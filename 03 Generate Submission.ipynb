{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b0c2e3-b892-4b56-9a23-d631dcb8043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "sensor_mask = [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23,\n",
    "                       45, 46, 47, \n",
    "                       120, 121, 122,\n",
    "                       138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, \n",
    "                       174, 175, 176, 177, 178, 179, 180, 181, 182,\n",
    "                       198, 199, 200,\n",
    "                       271, 272, 273, 274, 275, 276]\n",
    "\n",
    "with h5py.File('./libribrain/COMPETITION_HOLDOUT/derivatives/serialised/sub-0_ses-2025_task-COMPETITION_HOLDOUT_run-1_proc-bads+headpos+sss+notch+bp+ds_meg.h5', 'r') as h5_file:\n",
    "    # List all keys (data structure)\n",
    "    print(\"Keys:\", list(h5_file.keys()))\n",
    "    scaling_factor = 1e-07\n",
    "    # Select the dataset to extract (e.g. 'data')\n",
    "    data = h5_file['data'][:].astype(np.float64)  # use [:] extract as ndarray\n",
    "    masked_data = data[sensor_mask]\n",
    "    normalized = masked_data / scaling_factor\n",
    "# Save as .npy file to local disk\n",
    "np.save('./libribrain/detection_holdout_48.npy', normalized.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528a271a-a641-48db-996b-d855a6ee52cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "# Assuming classification_model.py is in the same directory or accessible via PYTHONPATH\n",
    "from seanet_style import SEANetTransformerClassifier, create_mask\n",
    "\n",
    "# --- Dataset for Single File Evaluation with Overlap ---\n",
    "class EvaluationDataset(Dataset):\n",
    "    def __init__(self, input_filepath, chunk_size=2000, overlap=0):\n",
    "        # Load the single input file using memory map\n",
    "        self.input_mmap_full = np.load(input_filepath, mmap_mode='r')\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "        self.stride = chunk_size - overlap\n",
    "        if self.stride <= 0:\n",
    "            raise ValueError(\"Overlap must be less than chunk_size.\")\n",
    "\n",
    "        self.data_chunks = []\n",
    "        self.chunk_start_indices = [] # To keep track of original positions\n",
    "\n",
    "        total_input_len = self.input_mmap_full.shape[1]\n",
    "\n",
    "        for start_idx in range(0, total_input_len, self.stride):\n",
    "            end_idx = min(start_idx + chunk_size, total_input_len)\n",
    "            \n",
    "            # Adjust start_idx for the last chunk to ensure it is full-sized if possible\n",
    "            if end_idx - start_idx < chunk_size and end_idx == total_input_len:\n",
    "                start_idx = max(0, total_input_len - chunk_size)\n",
    "\n",
    "            input_chunk = torch.from_numpy(self.input_mmap_full[:, start_idx:end_idx]).float()\n",
    "            \n",
    "            # Pad the last chunk if it's smaller than chunk_size\n",
    "            current_chunk_len = input_chunk.shape[1]\n",
    "            if current_chunk_len < chunk_size:\n",
    "                padding_needed = chunk_size - current_chunk_len\n",
    "                # Pad only the sequence dimension\n",
    "                input_chunk = F.pad(input_chunk, (0, padding_needed))\n",
    "            \n",
    "            self.data_chunks.append((input_chunk, input_chunk.shape[1]))\n",
    "            self.chunk_start_indices.append(start_idx) # Store original start index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_chunks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_chunk, original_len = self.data_chunks[idx]\n",
    "        start_idx = self.chunk_start_indices[idx]\n",
    "        return input_chunk, original_len, start_idx\n",
    "\n",
    "# --- Collate Function (adapted for new dataset output) ---\n",
    "def collate_fn(batch):\n",
    "    inputs, input_lengths_orig, start_indices = zip(*batch)\n",
    "    \n",
    "    input_lengths = list(input_lengths_orig)\n",
    "    \n",
    "    max_input_len = max(input_lengths)\n",
    "    # No padding needed here if EvaluationDataset already pads to chunk_size\n",
    "    # However, if it's dynamic chunking, this still applies.\n",
    "    # Given the updated EvaluationDataset, input_chunk will always be chunk_size\n",
    "    # so max_input_len will be chunk_size.\n",
    "    padded_inputs = torch.stack([x for x in inputs])\n",
    "    \n",
    "    return padded_inputs, input_lengths, list(start_indices)\n",
    "\n",
    "# --- Helper function to calculate Classification Metrics (F1-score) ---\n",
    "def calculate_classification_metrics(all_predictions_logits, all_targets, threshold=0.5):\n",
    "    all_predictions_probs = torch.sigmoid(torch.tensor(all_predictions_logits)).numpy()\n",
    "    all_predictions_binary = (all_predictions_probs >= threshold).astype(int)\n",
    "    all_targets_np = np.array(all_targets)\n",
    "\n",
    "    all_predictions_binary_flat = all_predictions_binary.flatten()\n",
    "    all_targets_np_flat = all_targets_np.flatten()\n",
    "\n",
    "    f1 = f1_score(all_targets_np_flat, all_predictions_binary_flat, zero_division=0)\n",
    "    precision = precision_score(all_targets_np_flat, all_predictions_binary_flat, zero_division=0)\n",
    "    recall = recall_score(all_targets_np_flat, all_predictions_binary_flat, zero_division=0)\n",
    "    \n",
    "    if len(np.unique(all_targets_np_flat)) > 1:\n",
    "        roc_auc = roc_auc_score(all_targets_np_flat, all_predictions_probs.flatten())\n",
    "    else:\n",
    "        roc_auc = float('nan')\n",
    "\n",
    "    return f1, precision, recall, roc_auc\n",
    "\n",
    "def infer_single_file(model, data_file_path, output_dir, device, \n",
    "                      chunk_size, overlap_size, batch_size, detection_threshold):\n",
    "    \n",
    "    base_filename = os.path.basename(data_file_path)\n",
    "    \n",
    "    # Load the entire numpy array\n",
    "    full_data = np.load(data_file_path)\n",
    "    # Ensure it's (channels, time) or (time, channels)\n",
    "    # Assuming full_data is (time_steps, channels) or (channels, time_steps)\n",
    "    # If it's (time_steps, channels), transpose it to (channels, time_steps)\n",
    "    if full_data.shape[0] == 48 and full_data.ndim == 2: # Already (channels, time)\n",
    "        original_length = full_data.shape[1]\n",
    "    elif full_data.shape[1] == 48 and full_data.ndim == 2: # (time, channels)\n",
    "        full_data = full_data.T # Transpose to (channels, time)\n",
    "        original_length = full_data.shape[1]\n",
    "    elif full_data.ndim == 1: # (time,)\n",
    "        raise ValueError(\"Input data cannot be 1D. Expected (channels, time) or (time, channels).\")\n",
    "    else: # Already 3D, e.g., (1, channels, time) or (batch, channels, time)\n",
    "        # Assuming it's a single long sequence, taking the first one if batch exists\n",
    "        if full_data.ndim == 3:\n",
    "            full_data = full_data[0] # Take the first sequence if it's (1, C, T) or (B, C, T)\n",
    "            if full_data.shape[0] != 48: # Transpose if C is not first\n",
    "                full_data = full_data.T\n",
    "        if full_data.shape[0] != 48:\n",
    "             raise ValueError(f\"Input data has {full_data.shape[0]} channels. Expected 23 after potential transpose.\")\n",
    "        original_length = full_data.shape[1]\n",
    "\n",
    "    # Create chunks for inference\n",
    "    chunks = []\n",
    "    start_indices = []\n",
    "    \n",
    "    step_size = chunk_size - overlap_size\n",
    "    if step_size <= 0:\n",
    "        raise ValueError(\"Step size (chunk_size - overlap_size) must be positive.\")\n",
    "\n",
    "    for i in range(0, original_length, step_size):\n",
    "        start = i\n",
    "        end = min(i + chunk_size, original_length)\n",
    "        \n",
    "        # Pad if the last chunk is smaller than chunk_size\n",
    "        current_chunk = full_data[:, start:end]\n",
    "        current_length = current_chunk.shape[1]\n",
    "\n",
    "        if current_length < chunk_size:\n",
    "            # Pad with zeros to chunk_size\n",
    "            padding_needed = chunk_size - current_length\n",
    "            padded_chunk = np.pad(current_chunk, ((0, 0), (0, padding_needed)), mode='constant')\n",
    "            chunks.append(padded_chunk)\n",
    "            start_indices.append(start)\n",
    "            # This is the last chunk, so break\n",
    "            break \n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "            start_indices.append(start)\n",
    "\n",
    "    # Convert chunks to tensor dataset\n",
    "    class InferenceDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, chunks, start_indices):\n",
    "            self.chunks = chunks\n",
    "            self.start_indices = start_indices\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.chunks)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            chunk = torch.from_numpy(self.chunks[idx]).float()\n",
    "            # input_length for the model is the actual length of the data in the chunk (before padding)\n",
    "            # This is critical if the model uses original_lengths for internal calculations.\n",
    "            # When padding, input_length should be the length *before* padding.\n",
    "            # However, your model now trims/pads output, so the actual length of the tensor passed is fine.\n",
    "            # Let's just pass the chunk_size as input_length for simplicity with the padding.\n",
    "            return chunk, torch.tensor(chunk.shape[1], dtype=torch.long), torch.tensor(self.start_indices[idx], dtype=torch.long)\n",
    "\n",
    "    inference_dataset = InferenceDataset(chunks, start_indices)\n",
    "    evaluation_dataloader = torch.utils.data.DataLoader(\n",
    "        inference_dataset, batch_size=batch_size, shuffle=False, num_workers=0\n",
    "    )\n",
    "\n",
    "    fused_predictions_logits = np.zeros(original_length, dtype=np.float32)\n",
    "    prediction_counts = np.zeros(original_length, dtype=np.int32)\n",
    "\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(evaluation_dataloader, desc=f\"Inferring {base_filename} with overlap\")\n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            inputs_batch, input_lengths_batch, start_indices_batch = batch\n",
    "            inputs_batch = inputs_batch.to(device)\n",
    "\n",
    "            # Convert input_lengths_batch to a list of Python integers if your model expects it\n",
    "            # The model's forward method expects `original_lengths: tp.List[int]`\n",
    "            input_lengths_list = input_lengths_batch.tolist()\n",
    "\n",
    "            predictions_logits_batch = model(inputs_batch, original_lengths=input_lengths_list).cpu().numpy() # (batch_size, 1, chunk_size)\n",
    "\n",
    "            for i, start_idx in enumerate(start_indices_batch):\n",
    "                current_input_length = input_lengths_list[i] # Use the actual length passed to model\n",
    "                \n",
    "                # predictions_logits_batch[i, 0, ...] should be of length `current_input_length` (1200)\n",
    "                chunk_prediction_logits = predictions_logits_batch[i, 0, :current_input_length]\n",
    "                \n",
    "                end_idx = min(original_length, start_idx.item() + current_input_length) # .item() for scalar tensor\n",
    "                effective_chunk_len = end_idx - start_idx.item()\n",
    "                \n",
    "\n",
    "                fused_predictions_logits[start_idx.item():end_idx] += chunk_prediction_logits[:effective_chunk_len]\n",
    "                prediction_counts[start_idx.item():end_idx] += 1\n",
    "\n",
    "    # Avoid division by zero for positions not covered by any chunk (though unlikely with proper overlap)\n",
    "    prediction_counts[prediction_counts == 0] = 1\n",
    "\n",
    "    # Final averaging\n",
    "    full_predictions_logits = fused_predictions_logits / prediction_counts\n",
    "    \n",
    "    # Convert logits to probabilities\n",
    "    full_predictions_probs = torch.sigmoid(torch.from_numpy(full_predictions_logits)).numpy()\n",
    "\n",
    "    # Apply threshold for binary classification\n",
    "    binary_predictions = (full_predictions_probs >= detection_threshold).astype(np.int32)\n",
    "\n",
    "    # Save predictions\n",
    "    output_probs_filepath = os.path.join(output_dir, f\"prediction_probs_{base_filename}\")\n",
    "    np.save(output_probs_filepath, full_predictions_probs)\n",
    "    output_binary_filepath = os.path.join(output_dir, f\"prediction_binary_{base_filename}\")\n",
    "    np.save(output_binary_filepath, binary_predictions)\n",
    "\n",
    "    print(f\"Probabilities for {base_filename} saved to {output_probs_filepath}\")\n",
    "    print(f\"Binary predictions for {base_filename} saved to {output_binary_filepath}\")\n",
    "\n",
    "    print(f\"\\nInference complete for {data_file_path}.\")\n",
    "    print(f\"Shape of predicted binary output: {binary_predictions.shape}\")\n",
    "    print(f\"First 20 binary predictions: {binary_predictions[:20].tolist()}\")\n",
    "    print(f\"First 20 probabilities: {full_predictions_probs[:20].tolist()}\")\n",
    "    \n",
    "    return full_predictions_probs, binary_predictions\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    model_path = \"./libribrain/speech_detection_final.pth\"  # Path to your trained model\n",
    "    input_file_for_evaluation = './libribrain/detection_holdout_48.npy'  # Replace with the actual input file path\n",
    "    output_prediction_dir = \"./libribrain/predictions\"  # Directory to save the output prediction\n",
    "\n",
    "    chunk_size_for_inference = 1000  # Must match the chunking used during validation/inference\n",
    "    overlap_for_inference = 950     # New: Overlap size. E.g., 200 for 50% overlap if chunk_size is 400.\n",
    "    \n",
    "    num_input_channels = 48  # Number of channels in your input data\n",
    "    num_output_channels = 1   # Model outputs 1 channel for binary classification\n",
    "    detection_threshold = 0.5\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_prediction_dir, exist_ok=True)\n",
    "    \n",
    "    # Model parameters for SEANetTransformerClassifier\n",
    "    model_params = {\n",
    "        'input_channels': 48,\n",
    "        'sampling_rate': 250,\n",
    "        'encoder_dimension': 16, # Feature dimension after encoder\n",
    "        'encoder_n_filters': 8,  # Base number of filters\n",
    "        'encoder_ratios': [5, 5, 2], # Encoder downsamples by these ratios (reversed in encoder). Total downsample = 5*5*2 = 50.\n",
    "        'transformer_n_heads': 4, # Reduced heads for smaller dimension\n",
    "        'transformer_n_layers': 1, # Reduced layers for faster iteration\n",
    "        'transformer_dim_feedforward': 64, # Smaller feedforward dim\n",
    "        'transformer_dropout': 0.1,\n",
    "        'decoder_out_channels': 1, # Binary classification logits\n",
    "            \n",
    "        # SEANet-style parameters for Encoder/Decoder blocks\n",
    "        'activation': 'GELU', \n",
    "        'activation_params': {}, \n",
    "        'norm': 'InstanceNorm1d', # Using LayerNorm for simplicity\n",
    "        'norm_params': {}, \n",
    "        'n_residual_layers': 1, # Number of residual layers per stage\n",
    "        'kernel_size': 7, \n",
    "        'last_kernel_size': 7, \n",
    "        'residual_kernel_size': 3, \n",
    "        'dilation_base': 2, \n",
    "        'compress': 2, \n",
    "        'true_skip': False # Use conv shortcut in resnet block\n",
    "    }\n",
    "    model = SEANetTransformerClassifier(**model_params).to(device).float()\n",
    "\n",
    "    # Load trained model weights\n",
    "    if os.path.exists(model_path):\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        print(f\"Loaded model weights from {model_path}\")\n",
    "    else:\n",
    "        print(f\"Error: Model weights not found at {model_path}. Please train the model first or provide the correct path.\")\n",
    "        exit()\n",
    "\n",
    "    # Perform inference on the single file with overlap\n",
    "    print(f\"Starting inference for: {input_file_for_evaluation} with chunk_size={chunk_size_for_inference}, overlap={overlap_for_inference}\")\n",
    "    predicted_binary_output, predicted_probabilities = infer_single_file(\n",
    "        model, \n",
    "        input_file_for_evaluation, \n",
    "        output_prediction_dir,\n",
    "        device, \n",
    "        chunk_size_for_inference, \n",
    "        overlap_for_inference, # Pass the overlap value\n",
    "        batch_size = 128,\n",
    "        detection_threshold = 0.5\n",
    "    )\n",
    "    print(f\"\\nInference complete for {input_file_for_evaluation}.\")\n",
    "    print(f\"Shape of predicted binary output: {predicted_binary_output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf21e16-dbe8-440b-a220-d1ec0e917db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnpl.datasets import LibriBrainCompetitionHoldout\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "# First, instantiate the Competition Holdout dataset\n",
    "speech_holdout_dataset = LibriBrainCompetitionHoldout(\n",
    "    data_path=\"./libribrain\",  # Same as in the other LibriBrain dataset - this is where we'll store the data\n",
    "    tmax=0.8,             # Also identical to the other datasets - how many samples to return/group together\n",
    "    task=\"speech\"         # \"speech\" or \"phoneme\" (\"phoneme\" is not supported until Phoneme track launch)\n",
    ")\n",
    "\n",
    "predictions = np.load(\"./libribrain/predictions/prediction_probs_detection_holdout_48.npy\").reshape(-1, 1) \n",
    "\n",
    "print(predictions.shape)\n",
    "\n",
    "tensor=torch.from_numpy(predictions)\n",
    "\n",
    "speech_holdout_dataset.generate_submission_in_csv(\n",
    "    tensor,\n",
    "    \"./libribrain/holdout_speech_predictions.csv\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002e9394-caec-4b96-8da5-1a03fd4f8377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
